{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "saisynapseworkspace"
		},
		"saisynapseworkspace-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'saisynapseworkspace-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:saisynapseworkspace.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"LS_ADLSGen2_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'LS_ADLSGen2'"
		},
		"saisynapseworkspace-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://sai2616initialstorage.dfs.core.windows.net"
		},
		"LS_ADLSGen2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://sai2616initialstorage.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "RepartitionWork",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Repartition",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "SrvrlesSprkPool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Repartition')]",
				"[concat(variables('workspaceId'), '/bigDataPools/SrvrlesSprkPool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/saisynapseworkspace-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('saisynapseworkspace-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/saisynapseworkspace-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('saisynapseworkspace-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NormalTrigger')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Pipeline 1",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 7,
						"startTime": "2024-03-17T13:37:00",
						"timeZone": "India Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Pipeline 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Add as Admin')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--Provide access\n\n--Step1: Add a new user using mail Id in Synapse Workspae (Acess control tab using owner role)\n--Step2: Add a new user using mail Id in Synapse Studio (Acess control tab using owner role)\n--Step3: Add a new user using mail Id in Blob storage (Provide owner and blob data contributor roles)\n--Step4: Execute below command to provide permissions to dedicated sql pool\n\nCREATE USER [sirigirisaikumar0@gmail.com] from EXTERNAL PROVIDER;\nEXEC sp_addrolemember 'db_owner','sirigirisaikumar0@gmail.com'",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DedicatedSqlPool",
						"poolName": "DedicatedSqlPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Autogenerated code from right click on file')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'filesystem_sai2616initialstorage_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [filesystem_sai2616initialstorage_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://filesystem@sai2616initialstorage.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE dbo.NYCTrip (\n\t[DateID] int,\n\t[MedallionID] int,\n\t[HackneyLicenseID] int,\n\t[PickupTimeID] int,\n\t[DropoffTimeID] int,\n\t[PickupGeographyID] int,\n\t[DropoffGeographyID] int,\n\t[PickupLatitude] float,\n\t[PickupLongitude] float,\n\t[PickupLatLong] nvarchar(4000),\n\t[DropoffLatitude] float,\n\t[DropoffLongitude] float,\n\t[DropoffLatLong] nvarchar(4000),\n\t[PassengerCount] int,\n\t[TripDurationSeconds] int,\n\t[TripDistanceMiles] float,\n\t[PaymentType] nvarchar(4000),\n\t[FareAmount] numeric(19,4),\n\t[SurchargeAmount] numeric(19,4),\n\t[TaxAmount] numeric(19,4),\n\t[TipAmount] numeric(19,4),\n\t[TollsAmount] numeric(19,4),\n\t[TotalAmount] numeric(19,4)\n\t)\n\tWITH (\n\tLOCATION = 'NYCTripSmall.parquet',\n\tDATA_SOURCE = [filesystem_sai2616initialstorage_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.NYCTrip\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DedicatedSqlPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create table in Dedicated pool')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "CREATE TABLE [dbo].[employees]\n(\n    empId int NOT NULL,\n    empName NVARCHAR(50),\n    gender NVARHAR(10)\n)\nWITH\n(\n    DISTRIBUTION = HASH (empId),\n    CLUSTERED COLUMNSTORE INDEX\n)\nGO\n\n\nInsert into dbo.employees VALUES(1,'Sai','Male')\nInsert into dbo.employees VALUES(2,'Hari','Male')\nInsert into dbo.employees VALUES(3,'Narayana','FeMale')\nInsert into dbo.employees VALUES(4,'Koteshwaramma','FeMale')\n\n\nselect * from dbo.employees;\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DedicatedSqlPool",
						"poolName": "DedicatedSqlPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedicated_Aggregations')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "--https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-pool\n\nSELECT passenger_count as PassengerCount,\n      SUM(trip_distance) as SumTripDistance_miles,\n      AVG(trip_distance) as AvgTripDistance_miles\nINTO dbo.PassengerCountStats\nFROM  dbo.NYCTaxiTripSmall\nWHERE trip_distance > 0 AND passenger_count > 0\nGROUP BY passenger_count;\n\nSELECT * FROM dbo.PassengerCountStats\nORDER BY PassengerCount;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DedicatedSqlPool",
						"poolName": "DedicatedSqlPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedicated_CTAS')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "--CTAS creates a physical table. So needs a storage. So dedicated sql pool only possible.\n--Physical tables are not possible in Serverless\n--https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?view=azure-sqldw-latest\n\nCREATE TABLE [dbo].[employeesNew]\nWITH\n(\n    DISTRIBUTION = HASH (empName),\n    CLUSTERED COLUMNSTORE INDEX\n)\nselect * from dbo.employees\n\n\n\nCREATE TABLE [dbo].[employeesNew2]\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    CLUSTERED COLUMNSTORE INDEX\n)\nselect * from dbo.employees\n\n\n\n--- Select i to will not allow to ontrol the distribution and will always create ROUN_ROBIN distribution\nselect * \ninto dbo.employees3 \nfrom dbo.employees;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedicated_Create table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "-- Dataset file: \n-- https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkI4bXRXb0x0UEdJUDBsOFY3Zm1lQUVhSWhJUXxBQ3Jtc0ttRGVzelVuWFBlbE94WjVsR21IUVdOb2dza045ZTZEeUpMOGM3dVVLRmNYQzc1dEk0UjRBTEdjV3dvcktaQXI0a00xY1VFcDBpT1VSMmh3S0JnS19yRDZQcUxoekJLZkJCSm14T0FxNVFNcVJfTWlGSQ&q=https%3A%2F%2Fazuresynapsestorage.blob.core.windows.net%2Fsampledata%2FNYCTaxiSmall%2FNYCTripSmall.parquet&v=xipW6s7hg2E\n--Example Link:\n-- https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-pool\n\nIF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id \nWHERE O.NAME = 'NYCTaxiTripSmall' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.NYCTaxiTripSmall\n    (\n    [VendorID] bigint, \n    [store_and_fwd_flag] nvarchar(1) NULL, \n    [RatecodeID] float NULL, \n    [PULocationID] bigint NULL,  \n    [DOLocationID] bigint NULL, \n    [passenger_count] float NULL, \n    [trip_distance] float NULL, \n    [fare_amount] float NULL, \n    [extra] float NULL, \n    [mta_tax] float NULL, \n    [tip_amount] float NULL, \n    [tolls_amount] float NULL, \n    [ehail_fee] float NULL, \n    [improvement_surcharge] float NULL, \n    [total_amount] float NULL, \n    [payment_type] float NULL, \n    [trip_type] float NULL, \n    [congestion_surcharge] float  NULL\n    )\nWITH\n    (\n    DISTRIBUTION = ROUND_ROBIN,\n     CLUSTERED COLUMNSTORE INDEX\n     -- HEAP\n    )\nGO\n\nCOPY INTO dbo.NYCTaxiTripSmall\n(VendorID 1, store_and_fwd_flag 4, RatecodeID 5,  PULocationID 6 , DOLocationID 7,  \n passenger_count 8,trip_distance 9, fare_amount 10, extra 11, mta_tax 12, tip_amount 13, \n tolls_amount 14, ehail_fee 15, improvement_surcharge 16, total_amount 17, \n payment_type 18, trip_type 19, congestion_surcharge 20 )\nFROM 'https://sai2616initialstorage.dfs.core.windows.net/filesystem/NYCTripSmall.parquet'\nWITH\n(\n    FILE_TYPE = 'PARQUET'\n    ,MAXERRORS = 0\n    ,IDENTITY_INSERT = 'OFF'\n)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DedicatedSqlPool",
						"poolName": "DedicatedSqlPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedicated_External table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "--External tables an be created in both Serverless and Dediated Sql pools. \n--use abfss path for dedicated pool and https path in serverless\n-- https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?view=azure-sqldw-latest\n-- Hadoop type supported and performs process hadoop based type\n--https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#external-tables-in-dedicated-sql-pool-and-serverless-sql-pool\nCREATE DATABASE SCOPED CREDENTIAL [ADLS_credential]\nWITH IDENTITY='SHARED ACCESS SIGNATURE',  \nSECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'\nGO\n\nCREATE EXTERNAL DATA SOURCE AzureDataLakeStore\nWITH\n  -- Please note the abfss endpoint when your account has secure transfer enabled\n  ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,\n    CREDENTIAL = ADLS_credential ,\n    TYPE = HADOOP\n  ) ;\n\nCREATE EXTERNAL FILE FORMAT census_file_format\nWITH\n(  \n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\n\nCREATE EXTERNAL TABLE census_external_table\n(\n    decennialTime varchar(20),\n    stateName varchar(100),\n    countyName varchar(100),\n    population int,\n    race varchar(50),\n    sex    varchar(10),\n    minAge int,\n    maxAge int\n)  \nWITH (\n    LOCATION = '/parquet/',\n    DATA_SOURCE = population_ds,  \n    FILE_FORMAT = census_file_format\n)\nGO\n\nSELECT TOP 1 * FROM census_external_table\n\n\n\n--Use:\n-- Diretly query from sources using TSql\n--Store query results in ADLS/Blob using CETAS\n--Import data from ADLS and Create teables in Dediated pools ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL Authentication')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Admin accounts (found in Synapse workspace > properties)\n    -- 1. Server Admin --- user name and passowrd\n    --  2. Active Directory Admin -----inidviaul, AD Groups authentications\n\n    select * from sys.sql_logins;\n-- Serverless:\n-- https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/sql-authentication?tabs=serverless\n-- https://www.serverlesssql.com/user-permissions-in-serverless-sql-pools-external-tables-vs-views/\n\n\n-- Dedicated Pool:\n-- https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-authentication",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Serverless_CETAS')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Serverless"
				},
				"content": {
					"query": "--CETAS creates external table and stores the data in the ADLS , Blob or HDFS\n--https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=sql-server-ver16&tabs=powershell\n\n--Managed Identity:\n--Every resource like ADF/Synapse will have an ID alled Managed ID and will be same as the resource name.\n--This an be forund in access blade of eah resource.\n\n--External objects will be under a Schema only\ncreate schema NYCTAXI;\n\nuse demoDB;\n-- Example is based on AdventureWorks\nCREATE EXTERNAL TABLE NYCTAXI.PassengerCountStats\n    WITH (\n            LOCATION = 'filesystem/NYCTaxi/Aggdata/',\n            DATA_SOURCE = SaiExternalADLS,\n            FILE_FORMAT = ParquetFileFormat\n            ) AS\nSELECT PassengerCount,\n       SUM(TripDistanceMiles) as SumTripDistance,\n       AVG(TripDistanceMiles) as AvgTripDistance\n   FROM \n        OPENROWSET(\n        BULK 'https://sai2616initialstorage.dfs.core.windows.net/filesystem/NYCTripSmall.parquet',\n        FORMAT = 'PARQUET'\n    ) \n    AS [rows]\n        WHERE TripDistanceMiles > 0 AND PassengerCount > 0\n        GROUP BY PassengerCount\n        ORDER BY PassengerCount\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Serverless_External Data Source')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Serverless"
				},
				"content": {
					"query": "--Details\n-- Serverless never stores the data . It just stores metadata like External tables.\n-- The data still exists in storgare like ADLS and T-sqls are performed everytime when you run a query\n-- Acts as a Logial Data Warehouse\n\n-- External Datasoure are like connection strings used for Data virtualization and data load using PolyBase\n-- https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver16&tabs=dedicated\n\n\n\n-- credential is authentication information that is required to connect outside the Sql Server\n--https://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-ver16\n\n\n\n-- Before creating a credential, the DB must have a master key to protect the credential\n-- https://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-ver16\n\n\n\n\n\n\nCreate DATABASE demoDB;\n\nUse demoDB;\n-- Create a db master key if one does not already exist, using your own password.\nCREATE MASTER KEY ENCRYPTION BY PASSWORD='Hello@123';\n\n\n-- Create a database scoped credential.\nCREATE DATABASE SCOPED CREDENTIAL MyCredentials\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = 'sv=2022-11-02&ss=bfqt&srt=s&sp=rwdlacupyx&se=2024-03-18T12:59:30Z&st=2024-03-18T04:59:30Z&spr=https&sig=5%2FjmnJDaekf8IelHjkRmtN2HAn6YJozYz6OaKeM2Ark%3D';\n\n\nCREATE EXTERNAL DATA SOURCE SaiExternalADLS\nWITH\n  (  LOCATION = 'https://sai2616initialstorage.dfs.core.windows.net/' ,\n    -- [ [ , ] CONNECTION_OPTIONS = '<key_value_pairs>'[,...]]\n     CREDENTIAL = MyCredentials \n    -- [ [ , ] PUSHDOWN = { ON | OFF } ]\n  );\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demoDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Serverless_External File Fomat')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Serverless"
				},
				"content": {
					"query": "-- External File Format  is the prerequisite for the External table creation as it has infer the layout of teh data referenced.\n--Parquet and Delimiuted Text are only supported(May increase in future)\n--https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql?view=sql-server-ver16&tabs=delimited\n\nuse demoDB;\n--Create an external file format for PARQUET files.\nCREATE EXTERNAL FILE FORMAT ParquetFileFormat\nWITH (\n         FORMAT_TYPE = PARQUET\n      , DATA_COMPRESSION = \n        'org.apache.hadoop.io.compress.SnappyCodec' \n    );",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "demoDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Serverless_Firstquery')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Serverless"
				},
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://sai2616initialstorage.dfs.core.windows.net/filesystem/store_customers.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Temporary Tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-temporary\n--Available only for the session of dedicated or serverless pool(Query window are sepearte session)\n--Table name starts with #\n--gets stored system databses temp DB\n--can also the distribution \n--can be reated using CTAS statements\n\nselect * from tempDb.#tblPerson",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Types of External Tables')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\n--Hadoop type:\n    -- type parameter in Data Source is HAdoop\n    -- Uses java tech to read from external files \n    --Available only in Dediated pools \n--Native type\n    -- type parameter in Data Source is not metioned\n    -- Uses Azure's native C++ tech to read from external files\n\n\n--Native external tables are good interms of performane as it takes less time to read the files when compared to Hadoop type extenal table",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Repartition')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SrvrlesSprkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "57a837a6-6fe8-43ce-acf4-b6ad60d42459"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/0fc4d8e8-8f21-4bd6-a59d-692b2a87791e/resourceGroups/SaiSynapseResourceGroup/providers/Microsoft.Synapse/workspaces/saisynapseworkspace/bigDataPools/SrvrlesSprkPool",
						"name": "SrvrlesSprkPool",
						"type": "Spark",
						"endpoint": "https://saisynapseworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SrvrlesSprkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM `nyctaxi`.`passengercountstats`\")\n",
							"# df.show(10)\n",
							"df.repartition(1)\n",
							"df.write.mode(\"overwrite\").csv(\"abfss://filesystem@sai2616initialstorage.dfs.core.windows.net/passengercountstats/data.csv\")\n",
							"\n",
							"df.write.mode(\"overwrite\").parquet(\"abfss://filesystem@sai2616initialstorage.dfs.core.windows.net/passengercountstats/data.parquet\")"
						],
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessSpark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SrvrlesSprkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b9462225-0bc6-4af2-bf42-f46ff4a11fce"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/0fc4d8e8-8f21-4bd6-a59d-692b2a87791e/resourceGroups/SaiSynapseResourceGroup/providers/Microsoft.Synapse/workspaces/saisynapseworkspace/bigDataPools/SrvrlesSprkPool",
						"name": "SrvrlesSprkPool",
						"type": "Spark",
						"endpoint": "https://saisynapseworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SrvrlesSprkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://filesystem@sai2616initialstorage.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS nyctaxi\")\r\n",
							"df.write.mode(\"overwrite\").saveAsTable(\"nyctaxi.trip\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.sql(\"SELECT * FROM nyctaxi.trip\") \r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.sql(\"\"\"\r\n",
							"   SELECT PassengerCount,\r\n",
							"       SUM(TripDistanceMiles) as SumTripDistance,\r\n",
							"       AVG(TripDistanceMiles) as AvgTripDistance\r\n",
							"   FROM nyctaxi.trip\r\n",
							"   WHERE TripDistanceMiles > 0 AND PassengerCount > 0\r\n",
							"   GROUP BY PassengerCount\r\n",
							"   ORDER BY PassengerCount\r\n",
							"\"\"\") \r\n",
							"display(df)\r\n",
							"df.write.saveAsTable(\"nyctaxi.passengercountstats\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df2 = spark.sql(\"select * from nyctaxi.passengercountstats\")\r\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SrvrlesSprkPool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DedicatedSqlPool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dedicated_IDENTITY to create Surrogatekey')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Dedicated Sql Pool"
				},
				"content": {
					"query": "--Surrogate key is a unique identifier supported only for tables. so only for dediated pools\n--https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity\n--60 distributions will be present for hash distribution\n--IDENTITY doesnt guaramtee the order of id\n--You can define the table using IDENTITY\n--IDENTITY column cannot be distribution key \n--IDENTITY column cannot be used when column is not and INT or BIGINT\n--IDENTITY column cannot be used for external tables\n\nCREATE TABLE dbo.T1\n(    C1 INT IDENTITY(1,1)    NOT NULL\n,    C2 VARCHAR(30)                NULL\n)\nWITH\n(   DISTRIBUTION = HASH(C2)\n,   CLUSTERED COLUMNSTORE INDEX\n)\n;\n\nINSERT INTO dbo.T1\nVALUES (NULL);\n\nINSERT INTO dbo.T1\nVALUES (NULL);\n\nSELECT *\nFROM dbo.T1;\n\nDBCC PDW_SHOWSPACEUSED('dbo.T1'); --> gives partition id for each record",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OPENROWSET')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- https://learn.microsoft.com/en-us/sql/t-sql/functions/openrowset-transact-sql?view=sql-server-ver16\n\n---doesnot require authentication when the ADLS is part of linked services in ADLS\n--- Else it requires authentication or should add the Storage Data ontributor Role for the Syanpse usr managed ID in the ADLS ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Apache spark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SrvrlesSprkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "56e68a56-af42-402d-b04f-b0b9dd9badaf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/0fc4d8e8-8f21-4bd6-a59d-692b2a87791e/resourceGroups/SaiSynapseResourceGroup/providers/Microsoft.Synapse/workspaces/saisynapseworkspace/bigDataPools/SrvrlesSprkPool",
						"name": "SrvrlesSprkPool",
						"type": "Spark",
						"endpoint": "https://saisynapseworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SrvrlesSprkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Pandas to read/write Azure Data Lake Storage Gen2 data in Apache Spark pool**\r\n",
							"\r\n",
							"https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/tutorial-use-pandas-spark-pool\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data file from URI of default Azure Data Lake Storage Gen2\r\n",
							"\r\n",
							"import pandas\r\n",
							"\r\n",
							"#read csv file\r\n",
							"# df = pandas.read_csv('abfs[s]://file_system_name@account_name.dfs.core.windows.net/file_path')\r\n",
							"df = pandas.read_csv('abfss://filesystem@sai2616initialstorage.dfs.core.windows.net/store_customers.csv',\r\n",
							"storage_options={'acount_key':'6hs6nsCtjDZz0B91nGen/+kOxv1hsaaCPF5MF2aBdtjq2zDwjRGT2eRbYjuDGLImjzQnnYLLz2q9+AStT2U+rQ=='})\r\n",
							"print(df)\r\n",
							"\r\n",
							"#write csv file\r\n",
							"data = pandas.DataFrame({'Name':['A', 'B', 'C', 'D'], 'ID':[20, 21, 19, 18]})\r\n",
							"data.to_csv('abfss://filesystem@sai2616initialstorage.dfs.core.windows.net/write_data.csv')"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data file from FSSPEC short URL of default Azure Data Lake Storage Gen2\r\n",
							"##Account Key authentication is poonly supported fro Linked service to use this mechanism of reading as of now\r\n",
							"\r\n",
							"import pandas\r\n",
							"\r\n",
							"#read data file\r\n",
							"df = pandas.read_csv('abfss://filesystem/store_customers.csv', storage_options ={'linked_service' :'LS_ADLSGen2'})\r\n",
							"print(df)\r\n",
							"\r\n",
							"#write data file\r\n",
							"data = pandas.DataFrame({'Name':['A', 'B', 'C', 'D'], 'ID':[20, 21, 19, 18]})\r\n",
							"data.to_csv('abfss://filesystem/writeDataUsingFSSPEC.csv', storage_options = {'linked_service' :'LS_ADLSGen2'})"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Use temp tables t referene data across multiple langues in a single notebook"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark \r\n",
							"val scala_df = spark.read.sqlanalytics(\"DedicatedSqlPool.dbo.NYCTaxiTripSmall\")\r\n",
							"scala_df.show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"#Does not wpork like this\r\n",
							"display(scala_df)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\r\n",
							"scala_df.createOrReplaceTempView(\"PersonTableTemporary\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"#now it works\r\n",
							"spark.sql(\"select * from PersonTableTemporary\")"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_ADLSGen2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_ADLSGen2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('LS_ADLSGen2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Import other notebooks')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SrvrlesSprkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3e356ea2-c22e-4bcc-8405-63495d654d56"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/0fc4d8e8-8f21-4bd6-a59d-692b2a87791e/resourceGroups/SaiSynapseResourceGroup/providers/Microsoft.Synapse/workspaces/saisynapseworkspace/bigDataPools/SrvrlesSprkPool",
						"name": "SrvrlesSprkPool",
						"type": "Spark",
						"endpoint": "https://saisynapseworkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SrvrlesSprkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		}
	]
}